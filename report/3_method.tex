\section{Method}
The following section describes how the different components of the simulator work. It also
includes a more detailed description of the the MESI and Dragon protocols.

\subsection{Components}
Figure~\ref{fig:overview} shows an overview of how the different components in the simulation are
structured. A loader component unpacks a zip archive and passes each file as a
\emph{record} stream to a corresponding core in the system. Each core has one L1 cache which is connected
to the central, shared bus.

\begin{figure}[ht]
    \centering
    \incfig{overview}
    \caption{Overview}
    \label{fig:overview}
\end{figure}

\subsubsection{Record}

A record consists of a label and a value. During the initialization of the simulation, the loader
decompresses and unpacks the input zip archive and converts each contained file to a stream of
records. Given the following line in one such an input file
\begin{lstlisting}
0 0x817ae8
\end{lstlisting}%
a record will be created in the following form:
\begin{lstlisting}
Record {
    label: Label::Load,
    value: 0x817ae8
}
\end{lstlisting}

\subsubsection{System}\label{sssection:system}

The system maintains the overall state of all the cores and updates the cores' internal states every
cycles. During creation, the system initializes a new core for each supplied record stream.

Updating the cores' internal state consists of three distinct stages: \texttt{step}, \texttt{snoop}
and \texttt{after\_snoop}. The execution of these stages is synchronized between cores, meaning that
all cores have to be finished with one stage until any core can enter the next stage.

The \texttt{step} stage lets the core parse the next instruction from the record steam (if the core
is currently not busy) and updates the core's cache's internal state appropriately, without any
information of the other caches. This means that e.g. all bus reads are expected to result in the
``exclusive'' state.

The \texttt{snoop} stage lets the core's cache snoop on the bus.
If the bus currently has an active task from one of the other cores, the cache may update its
internal state given the active task and current state of the affected cache line. This stage is
responsible for altering the current bus transaction if e.g.\ the requested address is shared, to
inform the requesting cache of its state.

The \texttt{after\_snoop} stage is a cleanup stage that is executed after the snooping stage.
It is required, because caches might have assumed wrong conditions during the initial \texttt{step}
phase. These caches may use this phase to update their internal state based on their (now altered)
active bus transaction. This stage e.g.\ turns a previously wrongly assumed ``exclusive'' state to
the ``shared'' state (MESI) if the bus transaction was changed by another core's cache to be a
shared read.

\subsubsection{Core}
The core maintains the state of its L1 cache and its record stream.

\subsubsection{Cache}
The cache stores the state of each cache line, the corresponding LRU values, which
protocol is in use, as well as the address layout of the cache. This layout is computed during
initialization of the cache. It uses the block-offset length, the index length, the tag length,
the cache-set size and the block size to provide methods for calculating the tag and index of a
given address. It is also used to convert between two cache representations, where one is made of
nested vectors that follow the idea of a cache that contains cache sets that contain the cache
lines. The other representation that we use is similar to the nested version, but flattened into a
single vector of contiguous elements.

The cache lines and LRU are both stored as a two dimensional vector containing an unsigned integer,
where the rows represent the sets and the columns represent the cache lines. Note that there is no need
to actually index into the cache lines using the block offset, because we only simulate the cache
without actually storing the cached values.

During the \texttt{step} phase (\ref{sssection:system}) of a core, the cache's update function is
called, if the core is not currently stalled by an ``other''-record. This update function first checks if
the bus is currently processing a transaction that belongs to this cache. The cache will return
immediately if this is the case. The cache therefore blocks further execution of new instructions
until its last transaction is finished. If this is not the case, the cache starts to execute the
next scheduled load or store instruction, if there is any. To execute a load, the cache searches
for the given address to see if it is already present in the cache (hit). In case of a cache miss,
it takes the necessary steps to load the cache block from main memory or one of the other caches.
If the new cache block's cache set is already full, a write-back of the evicted cache block is
commissioned. A store instruction also causes the cache to search for the given address to see if it
present in the cache. If not, a read or read-exclusive (depending on the protocol) is issued,
because of the write-allocate policy. Any required bus transactions are either directly put on the
bus or delayed if the bus is currently already used by another cache's transaction.

\subsubsection{Bus}

The bus is central storage object that contains one storage slot for a bus \emph{task}.
This limits the bus to only work on one task at one time. A task has the following form:

\begin{lstlisting}
Task {
    issuer_id: usize,
    remaining_cycles: usize,
    action: BusAction,
}
\end{lstlisting}

The task contains the ID of the core whose cache issued the task, the remaining clock cycles until
the task is finished and the type of bus action. There are a couple of bus actions which are shared
between both protocols, however a bus action like \texttt{BusUpdShared} is only valid for the
Dragon protocol. These protocol specific actions are only issued by the corresponding protocol and
are otherwise ignored. The bus actions are
represented in the following form:

\begin{lstlisting}
BusAction {
    BusRdMem(address, n_bytes),
    BusRdShared(address, n_bytes),
    BusRdXMem(address, n_bytes),
    BusRdXShared(address, n_bytes),
    BusUpdMem(address, n_bytes),
    BusUpdShared(address, n_bytes),
    Flush(address, n_bytes),
}
\end{lstlisting}

Some of the bus transactions have been specialized into two distinct forms, a memory and a shared
form. This is used to indicate whether the read or update only concerns memory (and therefore takes
more cycles) or if it goes to one of the other caches, possibly reducing the required time.

Each bus action state contains an address and the number of bytes that should be transmitted. Each
bus update cycle (which corresponds to one cycle), the bus reduces the number of remaining cycles
of the currently active bus transaction, if there is one.

\subsubsection{Protocol}\label{sec:sub_protocol}

The protocol is implemented as a Rust \emph{trait}, which defines shared behavior. This is very
similar to how interfaces work in other languages (e.g.\ Java), with some minor differences. For
example, traits cannot declare fields. The following methods are defined by the protocol trait:
\begin{lstlisting}
read()
write()
snoop()
after_snoop()
writeback_required()
invalidate()
is_shared()
\end{lstlisting}
These operations are required for the implementation of the MESI protocol and the Dragon protocol.
As briefly described earlier in the cache section, the cache keeps track of the current protocol in
use. When the cache is doing operations on specific cache lines, it invokes the proper method for
the underlying protocol as defined in the trait. Thus, the cache only stores a reference to a protocol
\begin{lstlisting}
protocol: Box<dyn Protocol>
\end{lstlisting}
and operations are invoked on this instance:
\begin{lstlisting}
protocol.snoop(...)
\end{lstlisting}
\,.

This is really useful, because the cache does not need to know which protocol is used and can
therefore be built more abstract.

\subsection{Protocol}
Like described in Section~\ref{sec:sub_protocol}, each protocol needs to implement a set of methods
to satisfy the protocol trait. The following two sub sections will go in depth how the MESI protocol and
Dragon protocol implements these, as well as show the state diagrams that come up.

\subsubsection{MESI}

A transition diagram for our implementation of the \texttt{step} phase for the MESI protocol can be found in Figure~\ref{fig:mesi}.
However, this diagram is a bit different from the transition diagram found on
Wikipedia~\cite{mesi_wiki}, because it only describes our first transition phase.
The transition diagram found on Wikipedia has a transition from the invalid state (I) to the shared
state (S), which cannot be found Figure~\ref{fig:mesi}.

This among all other transitions will be described in this section.

% TODO: describe cache hit for I.

\begin{figure}[H]
    \centering
    \incfig{mesi}
    \caption{MESI. The step phase of the update cycle.}\label{fig:mesi}
\end{figure}

$I \to S$.
The transition from I to S is instead modeled using the step phase (see Figure~\ref{fig:mesi}) and the after snoop phase (see Figure~\ref{fig:mesi_after_snoop}).
When a cache miss occurs, the cache line is moved from invalid (I) to exclusive (E) as seen in Figure~\ref{fig:mesi}.
This will issue a bus transaction as a miss occurred.
Another core that is currently holding the same cache line will snoop the bus and change the transaction to signal that it is currently holding the same cache line.
The core that issued the bus transaction will in the after snoop phase acknowledge the shared signal and change its state to S, see the transition from E to S in Figure~\ref{fig:mesi_after_snoop}.

$I \to E$.
When a cache miss occurs, the core will transition to E.
It will issue a bus transaction, but since no other core is holding the same cache line, the transaction will not be changed in the snoop phase, eliminating the chance of transitioning to S in the after snoop phase.

$I \to M$.
When a cache write miss occurs the core must first fetch the data due to write-allocate.
A write-allocate is simulated by stalling for the standard penalty amount of a read miss, while transitioning the cache line to state M as seen in Figure~\ref{fig:mesi}.
A \texttt{BusRdXMem} bus transaction will be issued.
Other cores that are snooping will notice this.
They will invalidate their cache lines and transition to I.

\begin{figure}[H]
    \centering
    \incfig{mesi_snoop}
    \caption{MESI. The snoop phase of the update cycle.}
    \label{fig:mesi_snoop}
\end{figure}

$S \to S$.
When a read hit occurs, no state transition will occur and no bus transation will be issued.
Thus, the state will stay in S.

$S \to M$.
When a cache hit occurs while a core is writing to a cache line that is in S, a transition to M will occur and a \texttt{BusRdXMem} bus transaction will be issued.
Other cores that are snooping will notice this.
They will invalidate their cache lines and transition to I.

$E \to E$.
When a cache hit occurs while a core is reading to a cache line that is in E, the cache line will remain in E and no bus transaction will be issued.

$E \to M$.
When a cache hit occurs while a core is writing to a cache line that is in E, a transition to M will occur but no bus transaction since the core can be sure that has the only copy of the cache line.

$M \to M$.
When a cache hit occurs while a core is writing or reading to a cache line that is in M, the cache line will remain in M and no bus transaction will be issued since it can be sure that no other core has a copy of the cache line.


\begin{figure}[H]
    \centering
    \incfig{mesi_after_snoop}
    \caption{MESI. The after snoop phase of the update cycle.}
    \label{fig:mesi_after_snoop}
\end{figure}

From a bus transaction, a cache line can either go to S or I by looking at the Figure~\ref{fig:mesi_snoop}.
When a \texttt{BusRdMem} occurs the cache line will go to state S, as it knows that some other core has currently accessed the same cache line.
This will as mentioned happen in the snoop phase.
The core that is changing the cache line to S will also change the type of the bus transaction to allow the scenario described for transition ($I \to S$).
Moreover, a core that is reading a cache line that another core has in its cache should be able to transition to S in the after snoop phase, while the other core should be able to switch to S in the snoop phase.
When a \texttt{BusRdXMem} occurs the cache line will go to state I, as it knows that some other core has currently written to the same cache line.
A cache line can also go to state I if it is evicted.
When a cache line in M is evicted or invalidated, it must be flushed.
A flush will result in stalling all cores until the operation is complete.

\subsubsection{Dragon}
A transition diagram for our implementation of the processor initiated transitions for the Dragon protocol can be found in Figure~\ref{fig:dragon}.
Compared to the implementation of the MESI protocol, the implementation of the Dragon is more impacted by the after snoop phase.
This is to make sure that the correct transitions occur when dealing with shared cache lines.
Since the Dragon protocol has two shared states, the transition diagram is more complicated.
Like the MESI protocol, our implementation does not line up directly with the transition diagram shown on Wikipedia~\cite{dragon_wiki}.
One noticeable difference is that there are no transitions from an ``invalid'' state to state Sc, Sm and M.
Like the previous section all transitions will be described in this section.
Even if ``invalid'' is not a state in the Dragon protocol, ``Any'' will denote a cache line that is not currently in E, Sc, Sm or M.
Thus, the cache line is guaranteed to miss in this ``state''.

% TODO: cache-to-cache

\begin{figure}[H]
    \centering
    \incfig{dragon}
    \caption{Dragon. The step phase of the update cycle.}
    \label{fig:dragon}
\end{figure}

$Any \to E$.
When a cache miss occurs while a core is reading to a cache line the cache line will enter state E.
A bus transaction will be issued, telling other cores that a read has happened.
No other core has a copy of the cache line, so the cache line will remain in state E.

$Any \to Sc$.
When a cache miss occurs while a core is reading to a cache line the cache line will enter state E.
A bus transaction will be issued, telling other cores that a read has happened.
At least one other core has a copy of the cache line, so these cores will during the snoop phase notice the bus transaction, and change their states accordingly.

$Any \to M$.
When a cache miss occurs while a core is writing to a cache line the cache line will enter state M.
A bus transaction will be issued, telling other cores that a read has happened.
No other core has a copy of the cache line, so the cache line will remain in state M.

$Any \to Sm$.
When a cache miss occurs while a core is writing to a cache line the cache line will enter state M.
A bus transaction will be issued, telling other cores that a write has happened.
At least one other core has a copy of the cache line, so these cores will during the snoop phase notice the bus transaction.
and change their states to Sc.
Any cache line that was in Sm or M will have to propagate the data.
This is represented as a penalty where all cores are stalled as no data is transmitted.

\begin{figure}[H]
    \centering
    \incfig{dragon_snoop}
    \caption{Dragon. The snoop phase of the update cycle.}
    \label{fig:dragon_snoop}
\end{figure}

$E \to M$

$Sc \to M$

$Sm \to M$


\begin{figure}[H]
    \centering
    \incfig{dragon_after_snoop}
    \caption{Dragon. The after snoop phase of the update cycle.}
    \label{fig:dragon_after_snoop}
\end{figure}

$M \to M$.
When a cache hit occurs while a core is writing or reading to a cache line in state M, the cache line will remain in state M and no bus transaction will be issued.

$E \to E$.
When a cache hit occurs while a core is reading to a cache line in state E, the cache line will remain in state E and no bus transaction will be issued.

$Sc \to Sc$.
When a cache hit occurs while a core is reading to a cache line in state Sc, the cache line will remain in state Sc and no bus transaction will be issued.

$Sm \to Sm$.
When a cache hit occurs while a core is reading to a cache line in state Sm, the cache line will remain in state Sm and no bus transaction will be issued.

\subsection{Testing}
To verify that all state transitions work, a set of unit tests for the MESI protocol and the Dragon protocol has been constructed.
The tests are manually created to test every possible state transition of both protocols.

These tests do not test all possible scenarios, though.
For example, in the MESI protocol a cache line can transition to state I if it is invalidated or evicted.
The tests implemented only invalidate a cache line to make sure that it goes to state I.

Some integration tests of smaller scale have been established as well to verify that all components work as intended.
These test only simulate a single-core program. This verifies that the cache
implementation is correct, while not necessarily verifying that the cache coherence protocol
implementation is correct.
